{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb8f3b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb7bc63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"api_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15f76f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8b4c844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 32.90it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader('f2', glob=\"**/*.txt\", show_progress=True)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c82a563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 essays loaded\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(docs)} essays loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c0ff3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your 1 documents have been split into 1 chunks\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print (f\"Your {len(docs)} documents have been split into {len(splits)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a9e583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'vectordb' in globals(): \n",
    "    vectordb.delete_collection()\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ad456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c059bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f58c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23dbb258",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Please predict the possible outcome and possible ways to resolve the case\"\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24018477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the potential outcomes of the case and what are some possible solutions to resolve it?', '2. Can you provide me with different scenarios for the outcome of the case and suggest potential resolutions?', '3. I would like to know the various possible results of the case and explore different approaches to resolve it. Could you help me with that?']\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c06b6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c762718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "077fbd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = llm.predict(text=PROMPT.format_prompt(context=unique_docs,question=question).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21b2fb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the information provided, the possible outcome of the case could be a favorable judgment for Ms. Jane Doe if she is able to provide sufficient evidence supporting her claims of gender-based discrimination and retaliation. If the court finds that TechCo Corporation indeed discriminated against Ms. Doe and wrongfully terminated her employment, possible ways to resolve the case could include financial compensation for damages, reinstatement of her employment, or a settlement agreement between the parties. However, without further information or legal analysis, it is difficult to determine the exact outcome or resolution of the case.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14162e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
